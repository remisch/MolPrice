{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deepchem.splits import RandomSplitter\n",
    "import wandb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "# setting random seeds for reproductibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "wandb.login()\n",
    "batch_size=64\n",
    "\n",
    "# Creating the model\n",
    "class MPNN(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, out_dim,\n",
    "                train_data, valid_data, test_data,\n",
    "                std, batch_size=32, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.std = std  # std of data's target\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        # Initial layers\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "        # Message passing layers\n",
    "        nn = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn, aggr='mean')\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "        # Readout layers\n",
    "        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "\n",
    "        # Initialization\n",
    "        x = self.atom_emb(data.x)\n",
    "        h = x.unsqueeze(0)\n",
    "        edge_attr = self.bond_emb(data.edge_attr)\n",
    "        \n",
    "        # Message passing\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))  # send message and aggregation\n",
    "            x, h = self.gru(m.unsqueeze(0), h)  # node update\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Readout\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.view(-1)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        #print(batch.y.shape)\n",
    "        self.log(\"Train loss\", loss, batch_size=self.batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        #print(f'validation{batch.y.shape}')\n",
    "        self.log(\"Valid MSE\", loss, batch_size=self.batch_size)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Test MSE\", loss, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class MCULE_DATA(InMemoryDataset):\n",
    "    # path to the data\n",
    "    path_to_data = '/datasets/mcule_purchasable_in_stock_prices_valid_smiles.csv'\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['mcule_purchasable_in_stock_prices_valid_smiles.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        # load raw data from a csv file\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "        smiles = df['SMILES'].values.tolist()\n",
    "        target = df['price 1 (USD)'].values.tolist()\n",
    "\n",
    "        # Convert SMILES into graph data\n",
    "        print('Converting SMILES strings into graphs...')\n",
    "        data_list = []\n",
    "        for i, smi in enumerate(tqdm(smiles)):\n",
    "\n",
    "            # get graph data from SMILES\n",
    "            graph = smiles2graph(smi)\n",
    "\n",
    "            # convert to tensor and pyg data\n",
    "            x = torch.tensor(graph['node_feat'], dtype=torch.long)\n",
    "            edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(graph['edge_feat'], dtype=torch.long)\n",
    "            y = torch.tensor([target[i]], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        # save data\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# create dataset\n",
    "\n",
    "\"\"\"For the creation of file to work, you need to:\n",
    "- create a folder called 'raw' in the dataset folder containing your raw data: here the file cleaned of bad smiles\n",
    "- it will generate a folder called processed with the data\n",
    "- put random seed 0 to be consistent in the splitting\n",
    "- Argument of the MCULE_DATA class is the folder where you can find the raw folder\n",
    "\"\"\"\n",
    "if __name__=='__main__':\n",
    "    dataset = MCULE_DATA('./datasets/').shuffle()\n",
    "\n",
    "\n",
    "    # split data\n",
    "    splitter = RandomSplitter()\n",
    "    train_idx, valid_idx, test_idx = splitter.split(dataset,frac_train=0.7, frac_valid=0.1, frac_test=0.2)\n",
    "    train_dataset = dataset[list(train_idx)]\n",
    "    valid_dataset = dataset[list(valid_idx)]\n",
    "    test_dataset = dataset[list(test_idx)]\n",
    "\n",
    "\n",
    "    mean = dataset.data.y.mean()\n",
    "    std = dataset.data.y.std()\n",
    "\n",
    "    #training the model\n",
    "\n",
    "    wandb.init(project=\"molprice\",\n",
    "            config={\n",
    "                \"batch_size\": 64,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"hidden_size\": 80,\n",
    "                \"max_epochs\": 60\n",
    "            })\n",
    "\n",
    "    gnn_model = MPNN(\n",
    "        hidden_dim=80,\n",
    "        out_dim=1,\n",
    "        std=std,\n",
    "        train_data=train_dataset,\n",
    "        valid_data=valid_dataset,\n",
    "        test_data=test_dataset,\n",
    "        lr=0.001,\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs = 1000,\n",
    "    )\n",
    "    wandb_logger = WandbLogger()\n",
    "\n",
    "    trainer.fit(\n",
    "        model=gnn_model,\n",
    "    )\n",
    "\n",
    "    results = trainer.test(ckpt_path=\"best\")\n",
    "    wandb.finish()\n",
    "\n",
    "    test_mse = results[0][\"Test MSE\"]\n",
    "    test_rmse = test_mse ** 0.5\n",
    "    print(f\"\\nMPNN model performance: RMSE on test set = {test_rmse:.4f}.\\n\")\n",
    "    torch.save(gnn_model.state_dict(), 'gnn_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
